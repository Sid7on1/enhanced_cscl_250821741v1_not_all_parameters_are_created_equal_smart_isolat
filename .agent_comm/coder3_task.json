{
  "agent_id": "coder3",
  "task_id": "task_3",
  "files": [
    {
      "name": "genetic_algorithm.py",
      "purpose": "Genetic algorithm implementation",
      "priority": "medium"
    },
    {
      "name": "gradient_methods.py",
      "purpose": "Gradient-based optimization",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.CL_2508.21741v1_Not_All_Parameters_Are_Created_Equal_Smart_Isolat",
    "project_type": "optimization",
    "description": "Enhanced AI project based on cs.CL_2508.21741v1_Not-All-Parameters-Are-Created-Equal-Smart-Isolat with content analysis. Detected project type: optimization (confidence score: 8 matches).",
    "key_algorithms": [
      "Single-Stage",
      "Cosine",
      "Better",
      "Rate",
      "Transfer",
      "All",
      "Guage",
      "Isolation",
      "Overall",
      "Parameter-Centric"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.CL_2508.21741v1_Not-All-Parameters-Are-Created-Equal-Smart-Isolat.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nNot All Parameters Are Created Equal: Smart Isolation Boosts\nFine-Tuning Performance\nYao Wang1, Di Liang2, Minlong Peng3\n1University of New South Wales\n2ByteDance Inc.\n3Fudan University\n{yao.wang11@student.unsw.edu.au, liangd17@fudan.edu.cn}\nAbstract\nSupervised fine-tuning (SFT) is a pivotal ap-\nproach to adapting large language models\n(LLMs) for downstream tasks; however, per-\nformance often suffers from the \u201cseesaw phe-\nnomenon\u201d, where indiscriminate parameter up-\ndates yield progress on certain tasks at the ex-\npense of others. To address this challenge,\nwe propose a novel Core Parameter Isolation\nFine-Tuning (CPI-FT) framework. Specifically,\nwe first independently fine-tune the LLM on\neach task to identify its core parameter re-\ngions by quantifying parameter update mag-\nnitudes. Tasks with similar core regions are\nthen grouped based on region overlap, forming\nclusters for joint modeling. We further intro-\nduce a parameter fusion technique: for each\ntask, core parameters from its individually fine-\ntuned model are directly transplanted into a\nunified backbone, while non-core parameters\nfrom different tasks are smoothly integrated via\nSpherical Linear Interpolation (SLERP), miti-\ngating destructive interference. A lightweight,\npipelined SFT training phase using mixed-task\ndata is subsequently employed, while freezing\ncore regions from prior tasks to prevent catas-\ntrophic forgetting. Extensive experiments on\nmultiple public benchmarks demonstrate that\nour approach significantly alleviates task in-\nterference and forgetting, consistently outper-\nforming vanilla multi-task and multi-stage fine-\ntuning baselines.\n1 Introduction\nLarge Language Models (LLMs) (Brown et al.,\n2020; Chowdhery et al., 2023; Raffel et al., 2020;\nTouvron et al., 2023) have demonstrated remark-\nable generalization across diverse natural language\ntasks, achieving impressive success on benchmarks\nspanning reasoning, dialogue, instruction follow-\ning, and more. SFT (Chung et al., 2024; Ouyang\n1Corresponding author: Minlong Peng.\n2This work was done while Yao Wang was interning at\nByteDance under Di Liang\u2019s supervision.et al., 2022; Sanh et al., 2021) remains a crucial\nmethodology for tailoring these models to specific\napplications, aligning them with human instruc-\ntions, and imbuing domain-specific expertise by\noptimizing on datasets of task-relevant examples.\nSupervised fine-tuning (SFT) faces significant\nchallenges in multi-task and multi-domain scenar-\nios. When applied to heterogeneous datasets, such\nas mathematical reasoning, creative writing, cod-\ning, and factual question answering, conflicting\noptimization objectives among tasks often lead to\nthe \"seesaw effect\" (Yu et al., 2020), where per-\nformance improvements on one task degrade oth-\ners. This issue hinders the development of robust,\nbroadly capable large language models (LLMs).\nExisting approaches, including joint multi-task fine-\ntuning, naive parameter sharing, and staged cur-\nricula (Ouyang et al., 2022; Caruana, 1997; Wei\net al., 2021), generally assume uniform parame-\nter importance across tasks, updating all parame-\nters indiscriminately. While multi-stage training\nalleviates direct gradient conflicts through sequen-\ntial task structuring, it remains a coarse-grained\nisolation strategy that exacerbates catastrophic for-\ngetting (McCloskey and Cohen, 1989; Kirkpatrick\net al., 2017), further eroding the model\u2019s ability to\ngeneralize across diverse tasks.\nWe hypothesize that the root cause of these chal-\nlenges lies in the phenomenon of parameter hetero-\ngeneity : distinct capabilities of large language mod-\nels (LLMs) rely on specific and potentially over-\nlapping subsets of parameters, with certain clus-\nters disproportionately contributing to particular\ntasks. Uniform updates across the entire parame-\nter space fail to account for the specialized roles\nof these localized parameter subsets, thereby fos-\ntering destructive interference among competing\ntasks (Chen et al., 2018). Mitigating such inter-\nference necessitates a paradigm shift from heuris-\ntic approaches or task-level isolation to a princi-\npled framework that explicitly models task sensi-arXiv:2508.21741v1  [cs.CL]  29 Aug 2025\n\n--- Page 2 ---\ntivities at the parameter level. Furthermore, achiev-\ning robust multi-task fine-tuning demands more\ngranular control over the fine-tuning process, en-\nabling task-specific optimization while maintaining\nmodel-wide coherence.\nMotivated by these observations, we introduce\nthe Core Parameter Isolation Fine-Tuning (CPI-\nFT) framework, featuring a novel parameter fusion\nmechanism specifically designed to systematically\nalleviate task interference and catastrophic forget-\nting in SFT. Our approach involves several key\nsteps. First, we independently fine-tune the LLM\non each task and identify a \u201ccore parameter region\u201d\nfor each, representing the parameter subsets most\ncrucial for the respective task. Next, we cluster\ntasks according to the overlap in their core param-\neter regions, grouping together tasks with similar\nparameter footprints that are more likely to bene-\nfit from joint adaptation with minimal conflict. In\nthe subsequent fusion stage, we select the model\nfrom the final training stage as a unified backbone.\nFor each task, we overwrite its corresponding core\nparameter region in the backbone with parameter\nvalues from its individually fine-tuned model, en-\nsuring reliable preservation of task-specific knowl-\nedge. For regions outside any task\u2019s core, we em-\nploy a SLERP-based (Spherical Linear Interpola-\ntion) parameter merging strategy: parameters are\nfirst normalized to unit vectors, and linear or spher-\nical interpolation is performed based on the angu-\nlar distance, enabling smooth and geometry-aware\nblending of distinct task knowledge while minimiz-\ning abrupt transitions and interference. Finally, we\nconduct a lightweight pipeline fine-tuning phase\non a mixed-task dataset, with previously identified\ncore parameter regions frozen, further consolidat-\ning the merged model\u2019s generalization capability.\nIn summary, this work makes the following con-\ntributions. First, we identify and articulate the\ncentral challenge of parameter heterogeneity in\nmulti-task supervised fine-tuning, emphasizing that\nna\u00efve uniform parameter adaptation is ill-suited for\naligning diverse and potentially conflicting task\nobjectives within LLMs. To overcome this, we\npropose a novel methodology that (i) empirically\nidentifies core parameter regions crucial to each\ntask through independent fine-tuning and update\nmagnitude analysis, (ii) leverages parameter region\noverlap for principled task grouping, and (iii) in-\ntroduces a task-aware parameter fusion scheme:\ntask-specific core parameter regions are directly\ntransferred from their respective models, whileother parameters are merged using a geometry-\naware SLERP-based interpolation. Further, a final\npipeline fine-tuning stage with core-region freez-\ning consolidates knowledge and ensures robust-\nness. Extensive experiments demonstrate that our\napproach consistently outperforms conventional\nmulti-task and multi-stage SFT baselines, substan-\ntially improving resistance to task interference and\ncatastrophic forgetting.\n2 Core Parameter Isolation Fine-Tuning\nThis section presents a detailed exposition of the\nproposed Core Parameter Isolation Fine-Tuning\n(CPI-FT) framework for supervised fine-tuning\n(SFT). CPI-FT is designed to address two prevalent\nchallenges in multi-task SFT: negative task inter-\nference and catastrophic forgetting. It achieves this\nby systematically identifying task-specific parame-\nter regions and preserving them through dynamic\nfreezing within a multi-stage training regime. The\nframework is grounded in the hypothesis of parame-\nter heterogeneity in large language models (LLMs),\nwhich posits that different tasks rely on distinct\nsubsets of model parameters. The overall CPI-FT\nworkflow is illustrated in Figure 1, and comprises\nthree core stages, detailed as follows.\n2.1 Formal Preliminaries and Setup\nWe consider a pre-trained Large Language Model\nMparameterized by \u03b8\u2208RD, with initial param-\neters \u03b8(0). Our goal is to adapt Musing a collec-\ntion of Ndiverse SFT tasks T={T1, T2, ..., T N}.\nEach task Tiis associated with a dataset Di=\n{(xj, yj)}|Di|\nj=1typically consisting of instruction-\nresponse pairs. The standard objective for fine-\ntuning on a single task Tiinvolves minimizing a\nloss function, usually the cross-entropy loss, over\nits corresponding dataset:\nLi(\u03b8) =\u22121\n|Di|X\n(x,y)\u2208DilogPM(\u03b8)(y|x) (1)\nOptimization is typically performed using stochas-\ntic gradient descent variants like Adam (Kingma\nand Ba, 2014). Standard multi-task SFT often\nminimizes a combined lossP\ni\u03bbiLi(\u03b8)or sam-\nples mini-batches from a mixture of datasetsS\niDi,\nupdating all parameters \u03b8.\n2.2 Stage 1: Identifying Task-Specific Core\nParameter Regions\nThe core premise of CPI-FT is that the functional\nspecialization required by different SFT tasks is\n\n--- Page 3 ---\nFigure 1: The figure illustrates task-specific core parameter isolation (a) and consolidated fine-tuning (b) approaches\nfor sequential training. In the isolation approach, each task is trained individually with its own core parameters,\nresulting in separate outputs. The models are then merged into a single model. In contrast, the timeline approach\ninvolves sequential training where tasks are processed in a sequence, followed by merging and tuning the model to\nisolate core parameters, ultimately producing a final unified model that generates outputs for all tasks.\nreflected in the differential utilization and adapta-\ntion of the LLM\u2019s parameters. To operationalize\nthis idea, we identify a task-specific core parameter\nregion by measuring the magnitude of parameter\nupdates induced during task-centric fine-tuning.\nRationale for Update Magnitude. We use the\nparameter update magnitude |\u03b8(i)\nj\u2212\u03b8(0)\nj|as the\ncriterion for importance, as it directly reflects the\ndegree to which a parameter deviates from its pre-\ntrained state to accommodate task Ti. This measure\nis computationally efficient and empirically identi-\nfies parameters that play a significant role in task\nadaptation (Kirkpatrick et al., 2017). In contrast,\nalternatives such as gradient magnitudes can be\nnoisy and transient, while second-order methods\n(e.g., the diagonal of the Fisher Information Ma-\ntrix as used in EWC (Kirkpatrick et al., 2017)) are\noften computationally prohibitive for large models\nand tend to capture sensitivity rather than actual\nchange.\nProcedure. For each task Ti\u2208 T, we perform\nan independent SFT run initialized from a shared\npre-trained checkpoint \u03b8(0). Fine-tuning is carried\nout exclusively on the task-specific dataset Difor a\nlimited number of steps or epochs ( Eprobe ). This\nprobing duration is chosen to induce meaningful\ntask-specific parameter shifts while avoiding full\nconvergence, which may saturate update magni-\ntudes or lead to overfitting. We denote the resulting\nparameters after probe fine-tuning as \u03b8(i).\n\u03b8(i)=SFT(M(\u03b8(0)),Di, Eprobe) (2)The absolute difference vector \u2206|\u03b8(i)| \u2208RD\n\u22650is\ncalculated element-wise:\n\u2206|\u03b8(i)\nj|=|\u03b8(i)\nj\u2212\u03b8(0)\nj|,forj= 1, ..., D (3)\nFor simplicity, we compute the update magnitude\nover all model parameters, including weights, bi-\nases, and normalization layer parameters. While\nthis holistic approach suffices for our current frame-\nwork, future work may explore more granular anal-\nyses based on layer-wise or parameter-type-specific\nimportance.\nCore Region Definition. The core parameter re-\ngion Cifor task Tiis defined as the set of in-\ndices corresponding to the parameters exhibiting\nthe largest update magnitudes. Specifically, we\nidentify the indices of the top p%of parameters\nranked by their update magnitude:\nCi=arg topkj\u2208{1..D}(\u2206|\u03b8(i)\nj|,\u230ap\u00b7D/100\u230b)(4)\nAlternatively, using a percentile threshold is equiv-\nalent. The hyperparameter pcontrols the size of\nthe core region; a smaller pleads to more focused\nbut potentially less comprehensive core regions.\n2.3 Stage 2: Task Grouping and Staging via\nCore Region Similarity\nTo structure the multi-stage SFT process, we group\ntasks according to the similarity of their identified\ncore parameter regions C1, . . . , C N. The underly-\ning hypothesis is that tasks exhibiting substantial\noverlap in their core parameter subsets are more\nprone to mutual interference when trained jointly,\nand may reflect related underlying capabilities.\n\n--- Page 4 ---\nSimilarity Measure. We quantify the overlap be-\ntween core regions CiandCjusing the Jaccard\nIndex, a standard measure for comparing finite sets:\nS(Ci, Cj) =|Ci\u2229Cj|\n|Ci\u222aCj|\u2208[0,1] (5)\nS(Ci, Cj) = 1 indicates identical core regions,\nwhile S(Ci, Cj) = 0 implies disjoint core regions.\nGrouping Strategy. We employ a simple yet ef-\nfective threshold-based clustering approach. Tasks\nTiandTjare considered sufficiently related to\nbe grouped together if their core region similar-\nityS(Ci, Cj)meets or exceeds a hyperparameter\nthreshold \u03c4\u2208[0,1].\nTi\u223cTj\u21d0\u21d2 S(Ci, Cj)\u2265\u03c4 (6)\nFinal task groups G1, G2, . . . , G K(with K\u2264N)\nare formed by computing connected components\nin a task similarity graph, where tasks TiandTj\nare connected if Ti\u223cTj. This transitive group-\ning ensures that if Ti\u223cTjandTj\u223cTk, then all\nbelong to the same group. The similarity thresh-\nold\u03c4controls grouping granularity: higher values\nyield smaller, more coherent groups, potentially\nincreasing training stages. We analyze CPI-FT\u2019s\nsensitivity to \u03c4in our experiments. While more\nadvanced clustering methods exist, connected com-\nponents offer a simple and efficient solution.\nStaging Order. Once the Ktask groups are\nformed, they must be ordered sequentially\n(G1, G2, ..., G K)to define the SFT stages. The\nordering can impact the final performance, as it\ndetermines the sequence of knowledge acquisition\nand parameter freezing. Potential strategies include\nusing a simple random ordering as a baseline, or-\ndering based on group size (e.g., training smaller\ngroups first or last), arranging groups according\nto task complexity if such metrics are available\n(akin to curriculum learning), or ordering groups\nto minimize the size of the frozen parameter set in\nthe initial stages. In this work, we primarily evalu-\nate random ordering and potentially one principled\nheuristic, leaving exhaustive exploration of optimal\nordering strategies for future investigation.\n2.4 Stage 3: Parameter Fusion Across Tasks\nThis stage introduces a parameter fusion mecha-\nnism to construct a unified model that integrates\ntask-specific knowledge from all task groups. Thefusion process selectively incorporates critical pa-\nrameters identified for each task, while applying\nsmooth interpolation in non-core regions to pre-\nserve model coherence.\nBase Model Selection. We begin by selecting\nthe model parameters derived from the last multi-\nstage fine-tuning group, \u03b8base, as the initial base\nmodel. This ensures the base model benefits from\ninformation accrued during multi-stage training.\nCore Parameter Overwrite. For each task Ti,\nidentified core parameter regions Ciare directly\noverwritten into the base model \u03b8baseusing the cor-\nresponding fine-tuned parameters from \u03b8(i):\n\u03b8fused,j=(\n\u03b8(i)\nj j\u2208Ci\n\u03b8base,jj /\u2208Ci(7)\nBy preserving task-specific critical regions, we en-\nsure that key capabilities for each task are fully\nretained in the final fused model.\nNon-Core Parameter Fusion. For parameters\noutside core regions ( j /\u2208Ci), smooth interpola-\ntion is critical to avoid abrupt inconsistencies or\nconflicts during fusion. We adopt a spherical linear\ninterpolation (SLERP) strategy, inspired by (God-\ndard et al., 2024), to blend these non-core regions:\n\u03b8fused,j=(\n\u03c9\u03b8(i)\nj+ (1\u2212\u03c9)\u03b8base,j,\u2220(\u03b8base,j, \u03b8(i)\nj)< \u03f5\nSLERP (\u03b8base,j, \u03b8(i)\nj, \u03c9),otherwise\n(8)\nwhere \u03c9is the interpolation factor, \u2220(\u00b7)is the an-\ngular distance between parameter vectors, and \u03f5is\na threshold for determining near-collinearity. If the\nvectors are nearly aligned ( \u2220< \u03f5), linear interpo-\nlation suffices; otherwise, SLERP ensures smooth\nblending using spherical geometry. This method\nbalances task-specific updates with overall model\ncoherence in non-critical areas.\n2.5 Stage 4: Consolidated Fine-Tuning via\nMulti-Stage Training\nFollowing parameter fusion, the final stage refines\nthe fused model \u03b8fusedvia a streamlined multi-stage\ntraining process. In contrast to earlier SFT stages,\nthis phase operates on sampled subsets of training\ndata and focuses on final calibration, while pre-\nserving task-specific parameter integrity through\ndynamic freezing.\nDynamic Freezing Mechanism. During fine-\ntuning, all core parameter regions identified in\n\n--- Page 5 ---\nthe earlier probe stage are frozen to preserve task-\nspecific knowledge and mitigate destructive inter-\nference. At each training stage k, the frozen param-\neter set consists of the union of core regions from\nall previously trained task groups:\nFk=k\u22121[\nl=1[\nTi\u2208GlCi (9)\nA binary mask Mkis constructed to define frozen\nand trainable parameters: Mk,j= 0 forj\u2208Fk,\nandMk,j= 1 otherwise. Updates are then re-\nstricted to unfrozen parameters during training:\n\u03b8t+1=\u03b8t+ \u2206\u03b8t\u2299Mk (10)\nwhere \u2206\u03b8tdenotes the gradient-based parameter\nupdate at training step t, and\u2299represents element-\nwise masking.\nSampled Data Calibration. Instead of using all\ntask data during this stage, we create a sampled\ndataset Dsample , which combines small proportions\nof data from each task group. The sampling ratio\nis chosen to ensure balanced representation across\ntasks while maintaining computational efficiency.\nThis careful selection prevents overfitting to large\nindividual datasets and allows for representative\ngradient updates across tasks.\nMulti-Stage Training Process. Fine-tuning\nproceeds sequentially across task groups\n{G1, G2, ..., G K}as derived in Stage 2. For each\ngroup Gk, data from the tasks in Gkis sampled\nto create D(k)\nstage, the training begins with the\nmodel parameters from the previous stage, \u03b8(k\u22121)\nstage .\nThe updates during this stage are guided by the\ndynamic freezing mechanism and the sampled\ndata:\n\u03b8(k)\nstage=Train(\u03b8(k\u22121)\nstage,D(k)\nstage, Mk) (11)\nHere, Train(\u00b7)signifies the training process, which\nminimizes the combined task-specific loss while\nrespecting the frozen mask Mk. At the end of all K\nstages, the final model parameters, \u03b8final, represent\nknowledge consolidated across all tasks:\n\u03b8final=\u03b8(K)\nstage (12)\nEfficiency and Robustness. By leveraging sam-\npled data and freezing task-specific core parame-\nter regions, the final fine-tuning pipeline reduces\ncomputational costs while preventing catastrophicforgetting. Dynamic freezing ensures protection of\ntask-critical knowledge, while sampled calibration\nbalances adaptability to new tasks and retention of\npreviously acquired capabilities.\n3 Results and Analysis\n3.1 Main Performance Comparison\nThe comparative performance results of Core Pa-\nrameter Isolation Fine-Tuning (CPI-FT) framework\nagainst baseline approaches across diverse tasks,\nmodels, and evaluation metrics are summarized in\nTable 1. This section analyzes the experimental\nfindings, highlights key insights, and verifies the ef-\nficacy of CPI-FT in addressing the core challenges\nof multi-task supervised fine-tuning.\nConsistent Outperformance Across Models and\nTasks Our method consistently outperforms all\nbaseline approaches across four distinct base\nmodels\u2014LLaMA-2-7B, Mistral-8B, Qwen1.5-7B,\nand Gemma-9B and five heterogeneous tasks:\nGSM8K (math reasoning), CodeAlpaca (code gen-\neration), LogiQA (logical reasoning), Alpaca (in-\nstruction tuning), and UltraChat (interactive dia-\nlogue). CPI-FT achieves the highest task-specific\nperformance in every experimental setting, as un-\nderscored by the bold results for individual tasks.\nFurthermore, CPI-FT achieves the best average nor-\nmalized score across all base model configurations,\ndemonstrating that its ability to mitigate task inter-\nference and catastrophic forgetting is both consis-\ntent and robust across model architectures.\nSuperiority of CPI-FT over Standard SFT Ap-\nproaches The full multi-task supervised fine-\ntuning (Full SFT) baseline\u2014where all model pa-\nrameters are updated uniformly across tasks with-\nout isolation\u2014consistently achieves the lowest per-\nformance across all tasks and model configurations.\nThis pronounced underperformance underscores\nthe detrimental effect of gradient conflicts inherent\nin na\u00efve fine-tuning over heterogeneous task mix-\ntures. In contrast, both the Random Multi-Stage\nand Heuristic Multi-Stage baselines yield moderate\nimprovements, supporting the intuition that tempo-\nrally separating task groups can partially mitigate\ninterference. However, even the strongest multi-\nstage heuristic consistently underperforms relative\nto CPI-FT. This performance gap reveals a key in-\nsight: temporal task scheduling alone is insufficient\nto resolve cross-task interference without explicit\nstructural parameter isolation.\n\n--- Page 6 ---\nBase Model Method GSM8K CodeAlpaca LogiQA Alpaca UltraChat Avg. Norm. Score\nLLaMA-2-7BFull SFT (Multi-task) 48.2 25.1 55.3 7.1 7.5 6.58\nMulti-Stage (Random, K=3) 49.5 24.8 56.0 7.3 7.6 6.70\nMulti-Stage (Heuristic) 50.1 25.5 56.8 7.0 7.4 6.75\nCPI-FT (Ours, p=1%, \u03c4=0.1) 53.5 27.2 59.1 7.6 7.8 7.21\nMistral-8BFull SFT (Multi-task) 46.5 24.0 53.8 6.9 7.3 6.37\nMulti-Stage (Random, K=3) 47.8 23.7 54.5 7.1 7.4 6.49\nMulti-Stage (Heuristic) 48.3 24.3 55.2 6.8 7.2 6.53\nCPI-FT (Ours, p=1%, \u03c4=0.1) 51.6 25.9 57.4 7.5 7.7 6.98\nQwen1.5-7BFull SFT (Multi-task) 49.8 26.0 56.5 7.3 7.7 6.79\nMulti-Stage (Random, K=3) 51.0 25.7 57.3 7.5 7.8 6.92\nMulti-Stage (Heuristic) 51.7 26.4 58.0 7.2 7.6 6.98\nCPI-FT (Ours, p=1%, \u03c4=0.1) 55.3 28.1 60.6 7.8 8.1 7.45\nGemma-9BFull SFT (Multi-task) 51.5 27.2 58.0 7.6 8.0 7.05\nMulti-Stage (Random, K=3) 52.8 26.9 58.9 7.8 8.1 7.19\nMulti-Stage (Heuristic) 53.5 27.6 59.7 7.5 7.9 7.26\nCPI-FT (Ours, p=1%, \u03c4=0.1) 57.2 29.4 62.5 8.1 8.4 7.73\nTable 1: The table presents the main performance comparison of different baselines on various SFT tasks. The\nmetric is represented by scores, where a higher score directly indicates a better model effect. For each individual\ntask, the best results achieved by any of the baselines are highlighted in bold. Additionally, the Avg. Norm. Score is\ncalculated by first normalizing the individual scores of each task to a consistent 0-10 scale, and then computing the\nmacro-average.\nCore Parameter Isolation Drives Robust Per-\nformance CPI-FT\u2019s gains can be attributed to\nits principled design: selectively identifying and\npreserving task-critical core parameter regions\nduring each stage of fine-tuning while ensuring\nsmooth blending of task-agnostic regions through\ngeometry-aware fusion mechanisms like SLERP.\nThis nuanced approach avoids the indiscriminate\noverwriting of parameters, a common pitfall in both\nFull SFT and Multi-Stage baselines. The results\nconfirm that addressing parameter heterogeneity at\na granular level is essential for aligning diverse task\nobjectives and avoiding catastrophic forgetting.\nCross-Model Generalization of CPI-FT The ro-\nbustness of CPI-FT is evident across wide range of\nbaselines with varying architectures and parameter\ncounts. For every model\u2014including LLaMA-2-7B,\nMistral-8B, Qwen1.5-7B, and Gemma-9B\u2014CPI-\nFT maintains its superior performance on all tasks,\noutperforming both multi-task and multi-stage\nbaselines. Notably, the gains are consistent regard-\nless of whether the model is derived from decoder-\nonly architectures or features unique design opti-\nmizations such as Qwen\u2019s advanced pre-trainingtechniques or Gemma\u2019s extended scale. This gener-\nalizability underscores that CPI-FT\u2019s foundations\nare model-independent, making it broadly applica-\nble across the spectrum of LLMs.\nAnalysis of Average Normalized Scores To en-\nable a fair comparison across tasks with varying\nmetric scales, we compute an average normalized\nscore by rescaling each task\u2019s performance to a\ncommon range (0\u201310) and then calculating the\nmacro-average across tasks. CPI-FT consistently\nattains the highest normalized scores across all\nmodel configurations, with improvements ranging\nfrom 6.96 (Mistral-8B) to 7.70 (Gemma-9B). No-\ntably, its performance gains are most pronounced\non tasks requiring complex reasoning, such as\nGSM8K (+3\u20135 points over Full SFT) and LogiQA\n(+2\u20134 points over Heuristic Multi-Stage). These\nresults suggest that CPI-FT\u2019s ability to identify and\npreserve task-critical parameter regions is particu-\nlarly beneficial for tasks that demand specialized\nreasoning capabilities.\nIn summary, CPI-FT delivers superior multi-task\nperformance by systematically addressing parame-\nter heterogeneity and mitigating task interference\n\n--- Page 7 ---\nMethodA\u2192B B \u2192A\n\u2206A (\u2193) \u2206B (\u2191) \u2206B (\u2193) \u2206A (\u2191)\nFull SFT \u221224.5 +13 .4 \u221216.7 +20 .2\nMulti-Stage SFT \u221216.2 +12 .6 \u221212.3 +17 .5\nDPI (Ours) \u22125.7 +12.2 \u22124.8 +18.8\nTable 2: Catastrophic forgetting analysis in sequential (A \u2192B) and reverse (B \u2192A) fine-tuning on LLaMA-2-7B. \u2206\nvalues indicate absolute score changes on the first and second task (A or B) after subsequently fine-tuning on the\nother. Lower (less negative) forgetting indicates stronger retention. Results are averaged over three runs; all metrics\nare mapped to a unified 0-100 scale for comparability.\nduring fine-tuning. Its principled design preserves\ntask-critical parameters and integrates them seam-\nlessly into a unified, general-purpose model. By\novercoming the limitations of na\u00efve multi-task SFT\nand heuristic multi-stage training, CPI-FT achieves\nstate-of-the-art results across a diverse set of tasks\nand base models, demonstrating both its effective-\nness and generalizability.\n4 Sequential Fine-Tuning Forgetting\nAnalysis\nWe select two prototypical and potentially conflict-\ning tasks: GSM8K (math reasoning) and Alpaca\n(open-ended instruction following). Each model\nis first fine-tuned on Task A for a fixed budget (5\nepochs), then on Task B for the same budget, with\nno access to Task A data in the second stage. We\nrepeat the experiment in reverse order. At each\nstage, performances on both tasks are recorded. Ta-\nble 2 reveals the degree of catastrophic forgetting\nexperienced by each method in a two-task transfer\nsetup. Standard Full SFT suffers severe forgetting,\nwith performances on the initial task dropping by\nover 16\u201324 points after the second task is intro-\nduced. Multi-Stage SFT, which separates updates\ntemporally, partially alleviates forgetting, but per-\nsistent degradation remains prominent. By contrast,\nDPI reduces forgetting by over 65%, with post-fine-\ntuning losses consistently below 6points in both\ndirections, dramatically narrowing the forgetting\ngap. Notably, DPI preserves strong adaptation to\nthe second task ( \u2206B/\u2206A positives align with or ex-\nceed baselines), suggesting that improved retention\nis not at the expense of new knowledge acquisition.\n5 Multi-Stage vs. Single-Stage Tuning\nwith Dynamic Freezing\nTo evaluate the necessity of our multi-stage dy-\nnamic freezing pipeline in the final consolidationphase (Stage 4), we compare it against a more\nstraightforward single-stage approach. In the multi-\nstage setup, task groups\u2014formed based on core pa-\nrameter overlap\u2014are integrated sequentially, with\nall core parameters frozen at once and non-core\nparameter regions updated in sequence for each\ngroup. In the single-stage variant, and the model\nis fine-tuned on the randomly shuffled union of all\nsampled task data in a single pass. Both strate-\ngies utilize identical sampled datasets and freez-\ning masks. As shown in Table 3, the multi-stage\nconsolidation outperforms the single-stage variant\nacross all tasks, with notable gains in the more\ninterference-prone benchmarks such as GSM8K\nand LogiQA. While the performance gap is not\nlarge, the multi-stage pipeline provides a consistent\nadvantage, supporting its utility for preserving task-\nspecific capabilities and mitigating catastrophic\nforgetting. However, the single-stage approach\nachieves reasonably strong performance with sim-\npler implementation, which may suffice in settings\nwhere training time is a key constraint.\n6 Robustness under\nResource-Imbalanced Scenarios\nTo assess the robustness of our proposed DPI frame-\nwork in realistic mixed-resource settings, we sim-\nulate a scenario where certain tasks have signifi-\ncantly less training data compared to others. Specif-\nically, we select four representative tasks: Task A\n(Text Classification), Task B (Natural Language In-\nference), Task C (Named Entity Recognition), and\nTask D (Code Generation). For each target task in\nturn, we create reduced versions of its dataset at\n50%, 20%, and 10% of the full size, while retain-\ning the full data for other tasks. We then conduct\nmulti-task training using both vanilla SFT and DPI,\nkeeping all other settings fixed, and report perfor-\nmance for both low-resource and high-resource\n\n--- Page 8 ---\nStrategy GSM8K CodeAlpaca LogiQA Alpaca Avg. Norm. Score\nMulti-Stage (Ours) 53.4 27.1 59.2 7.6 7.18\nSingle-Stage (Frozen) 51.9 26.5 58.1 7.4 7.01\nTable 3: Comparison of Multi-Stage vs. Single-Stage consolidation (LLaMA-2-7B). Multi-stage achieves higher\nscores across all benchmarks, but the single-stage variant is competitive.\nTaskVanilla SFT DPI (Ours)\n100% 50% 20% 10% 100% 50% 20% 10%\nTask A 92.1 87.2 78.0 68.5 92.3 89.0 82.4 74.1\nTask B 90.3 86.1 77.8 70.2 90.5 87.8 81.2 75.3\nTask C 88.4 80.7 74.3 65.9 88.1 82.0 78.1 70.8\nTask D 31.7 27.8 20.1 15.9 32.0 29.4 25.3 19.7\nTable 4: Performance (%) of Vanilla SFT vs. DPI under different data ratios. 100% denotes full data for a task,\nothers are under-sampled.\ntasks. Table 4 presents the results, showing that all\nmodels experience performance drops as the target\ntask\u2019s data decreases. However, DPI consistently\noutperforms vanilla SFT, especially under extreme\ndata scarcity. For example, at the 10% data level,\nDPI improves the average low-resource task score\nby 3.7 points compared to SFT. Meanwhile, high-\nresource tasks see no significant decline, confirm-\ning that DPI\u2019s core-region protection mechanism\neffectively safeguards low-resource tasks without\nsacrificing overall model performance. Notably,\nthe relative gain from DPI increases as the degree\nof imbalance grows, demonstrating its robustness\nin practical multi-task scenarios.\n7 Impact of Similarity Threshold ( \u03c4).\nWe evaluate the sensitivity of CPI-FT to the simi-\nlarity threshold \u03c4, which determines how tasks are\ngrouped based on core parameter region overlap.\nThis experiment was conducted across multiple\nbase models (LLaMA-2 7B, Mistral-7B, Qwen1.5-\n7B, Gemma-7B) with the core percentage fixed at\np= 1% . The results, measured by the average\nnormalized score, are presented in Figure 2. Re-\nsults reveal a consistent pattern across all base mod-\nels: task grouping based on core parameter simi-\nlarity ( \u03c4 >0) substantially outperforms no group-\ning (\u03c4= 0), which approximates standard multi-\ntask SFT. Performance generally peaks at a mod-\nerate threshold\u2014specifically, \u03c4= 0.1in our ex-\nperiments\u2014and gradually declines as \u03c4increases.\nWhile a very high threshold (e.g., \u03c4= 0.5) leadsto lower performance than the peak, it still outper-\nforms the no-grouping baseline. These findings\nsuggest that moderate task isolation encourages\nbeneficial separation without hindering cross-task\ngeneralization, whereas overly fine-grained group-\ning may limit model plasticity or restrict knowl-\nedge transfer between related tasks. Notably, the\noptimal threshold remains stable around \u03c4= 0.1\nacross model architectures, indicating it may serve\nas a robust default. These results validate the effec-\ntiveness of CPI-FT\u2019s data-driven grouping strategy\nover undifferentiated supervised fine-tuning.\n8 Conclusion\nIn this paper, we introduced Core Parameter Isola-\ntion Fine-Tuning(CPI-FT), a principled framework\nfor supervised fine-tuning (SFT) of large language\nmodels (LLMs) that mitigates task interference by\nidentifying and isolating task-specific core parame-\nter regions. By leveraging dynamic freezing during\nmulti-stage fine-tuning, CPI-FT preserves critical\nparameters for earlier tasks while enabling special-\nization for new ones. Extensive experiments on di-\nverse datasets demonstrated CPI-FT\u2019s effectiveness\nin addressing the \"seesaw effect\", reducing catas-\ntrophic forgetting, and consistently outperforming\nmulti-task and multi-stage fine-tuning baselines.\nThis work highlights the importance of parameter\nheterogeneity in SFT and provides a scalable ap-\nproach for robust task adaptation in heterogeneous\nscenarios, paving the way for future improvements\nin fine-tuning methodologies.\n\n--- Page 9 ---\nReferences\nAnthropic. 2024. Introducing contextual retrieval. Ac-\ncessed: 2025-02-01.\nVimal Aribandi, Christopher Clark, Mostafa Khabsa,\nand 1 others. 2021. Ext5: Towards extreme multi-\ntask scaling for transfer learning. In arXiv preprint\narXiv:2111.10952 .\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, and 1 others. 2020. Language models are\nfew-shot learners. Advances in neural information\nprocessing systems , 33:1877\u20131901.\nRich Caruana. 1997. Multitask learning. Machine\nlearning , 28:41\u201375.\nSahil Chaudhary. 2023. Code alpaca: An instruction-\nfollowing llama model for code generation.\nMichael Chen, Jan Tworek, Heewoo Jun, Qingqing\nYuan, Tom Jacobsen, Tijana Radulovic, Jungo Kasai,\nGuy Ephrath, and Nando De Freitas. 2020. Just ask\nfor general knowledge: Training universal retrieval\nmodels for open-domain question answering. arXiv\npreprint arXiv:2008.03886 .\nZhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and\nAndrew Rabinovich. 2018. Gradnorm: Gradient\nnormalization for adaptive loss balancing in deep\nmultitask networks. In International conference on\nmachine learning , pages 794\u2013803. PMLR.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, and 1 others. 2023. Palm: Scaling\nlanguage modeling with pathways. Journal of Ma-\nchine Learning Research , 24(240):1\u2013113.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert\nWebson, Shixiang Shane Gu, Zhuyun Dai, Mirac\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Alex\nCastro-Ros, Marie Pellat, Kevin Robinson, and 16\nothers. 2022. Scaling instruction-finetuned language\nmodels. Preprint , arXiv:2210.11416.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, and\n1 others. 2024. Scaling instruction-finetuned lan-\nguage models. Journal of Machine Learning Re-\nsearch , 25(70):1\u201353.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, and 1 others. 2021. Training verifiers\nto solve math word problems. arXiv preprint\narXiv:2110.14168 .Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 conference of the\nNorth American chapter of the association for com-\nputational linguistics: human language technologies,\nvolume 1 (long and short papers) , pages 4171\u20134186.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi\nZheng, Shengding Hu, Zhiyuan Liu, Maosong Sun,\nand Bowen Zhou. 2023. Enhancing chat language\nmodels by scaling high-quality instructional conver-\nsations. arXiv preprint arXiv:2305.14233 .\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2022.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. Preprint ,\narXiv:2101.03961.\nZichu Fei, Qi Zhang, Tao Gui, Di Liang, Sirui Wang,\nWei Wu, and Xuan-Jing Huang. 2022. Cqg: A sim-\nple and effective controlled generation framework\nfor multi-hop question generation. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 6896\u20136906.\nJonathan Frankle and Michael Carbin. 2019. The lottery\nticket hypothesis: Finding sparse, trainable neural\nnetworks. In International Conference on Learning\nRepresentations (ICLR) .\nElias Frantar and Dan Alistarh. 2023. Sparsegpt: Mas-\nsive language models can be accurately pruned in\none-shot. Preprint , arXiv:2301.00774.\nCharles Goddard, Shamane Siriwardhana, Malikeh\nEhghaghi, Luke Meyers, Vladimir Karpukhin, Brian\nBenedict, Mark McQuade, and Jacob Solawetz. 2024.\nArcee\u2019s mergekit: A toolkit for merging large lan-\nguage models. In Proceedings of the 2024 Confer-\nence on Empirical Methods in Natural Language\nProcessing: Industry Track , pages 477\u2013485.\nMichelle Guo, Albert Haque, De-An Huang, Serena\nYeung, and Li Fei-Fei. 2018. Dynamic task prioriti-\nzation for multitask learning. In Proceedings of the\nEuropean conference on computer vision (ECCV) ,\npages 270\u2013287.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Olivier de Laroussilhe, Andrea Ges-\nmundo, Giuseppe Attanasio, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In Inter-\nnational Conference on Machine Learning (ICML) .\nEdward J. Hu, Yelong Shen, Patrick Wallis, Zeyuan\nAllen-Zhu, Sanye Li, Lu Wang, and Liang Wang.\n2021. Lora: Low-rank adaptation of large language\nmodels. arXiv preprint arXiv:2106.09685 .\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris Bam-\nford, Devendra Singh Chaplot, Diego de las Casas,\nEmma Bou Hanna, Florian Bressand, and 1 others.\n2024. Mixtral of experts. arXiv e-prints , pages\narXiv\u20132401.\n\n--- Page 10 ---\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 .\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, and 1 others. 2017.\nOvercoming catastrophic forgetting in neural net-\nworks. Proceedings of the national academy of sci-\nences , 114(13):3521\u20133526.\nBo Li, Di Liang, and Zixin Zhang. 2024a. Co-\nmateformer: Combined attention transformer for\nsemantic sentence matching. arXiv preprint\narXiv:2412.07220 .\nLiang Li, Qisheng Liao, Meiting Lai, Di Liang, and\nShangsong Liang. 2024b. Local and global: Text\nmatching via syntax graph calibration. In ICASSP\n2024-2024 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , pages\n11571\u201311575. IEEE.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 4582\u2013\n4597, Online. Association for Computational Lin-\nguistics.\nZhizhong Li and Derek Hoiem. 2017. Learning without\nforgetting. Preprint , arXiv:1606.09282.\nDi Liang, Fubao Zhang, Qi Zhang, and Xuan-Jing\nHuang. 2019a. Asynchronous deep interaction net-\nwork for natural language inference. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP) , pages 2692\u20132700.\nDi Liang, Fubao Zhang, Weidong Zhang, Qi Zhang, Jin-\nlan Fu, Minlong Peng, Tao Gui, and Xuanjing Huang.\n2019b. Adaptive multi-attention network incorporat-\ning answer information for duplicate question detec-\ntion. In Proceedings of the 42nd international ACM\nSIGIR conference on research and development in\ninformation retrieval , pages 95\u2013104.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang,\nYile Wang, and Yue Zhang. 2020. Logiqa: A\nchallenge dataset for machine reading compre-\nhension with logical reasoning. arXiv preprint\narXiv:2007.08124 .\nXinda Liu and Lili Wang. 2024. Multi-granularity se-\nquence generation for hierarchical image classifica-\ntion. Computational Visual Media , 10(2):243\u2013260.\nYonghao Liu, Mengyu Li, Di Liang, Ximing Li,\nFausto Giunchiglia, Lan Huang, Xiaoyue Feng, and\nRenchu Guan. 2024. Resolving word vagueness with\nscenario-guided adapter for natural language infer-\nence. arXiv preprint arXiv:2405.12434 .Yonghao Liu, Di Liang, Fang Fang, Sirui Wang, Wei\nWu, and Rui Jiang. 2023a. Time-aware multiway\nadaptive fusion network for temporal knowledge\ngraph question answering. In ICASSP 2023-2023\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , pages 1\u20135. IEEE.\nYonghao Liu, Di Liang, Mengyu Li, Fausto Giunchiglia,\nXiming Li, Sirui Wang, Wei Wu, Lan Huang, Xi-\naoyue Feng, and Renchu Guan. 2023b. Local and\nglobal: Temporal question answering via information\nfusion. In IJCAI , pages 5141\u20135149.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le,\nBarret Zoph, Jason Wei, and 1 others. 2023. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. In International Conference on\nMachine Learning , pages 22631\u201322648. PMLR.\nDavid Lopez-Paz and Marc\u2019Aurelio Ranzato. 2022.\nGradient episodic memory for continual learning.\nPreprint , arXiv:1706.08840.\nDavid L\u00f3pez-Paz and Marc\u2019Aurelio Ranzato. 2017.\nGradient episodic memory for continual learning. In\nAdvances in Neural Information Processing Systems\n(NeurIPS) .\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101 .\nJinghui Lu, Dongsheng Zhu, Weidong Han, Rui Zhao,\nBrian Mac Namee, and Fei Tan. 2023. What makes\npre-trained language models better zero-shot learn-\ners? In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 2288\u20132303, Toronto,\nCanada. Association for Computational Linguistics.\nRuotian Ma, Yiding Tan, Xin Zhou, Xuanting Chen,\nDi Liang, Sirui Wang, Wei Wu, Tao Gui, and\nQi Zhang. 2022. Searching for optimal subword\ntokenization in cross-domain ner. arXiv preprint\narXiv:2206.03352 .\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learn-\ning and motivation , volume 24, pages 109\u2013165. Else-\nvier.\nSiyuan Mu and Sen Lin. 2025. A comprehensive sur-\nvey of mixture-of-experts: Algorithms, theory, and\napplications. Preprint , arXiv:2503.07137.\nBehnam Neyshabur, Ryota Tomioka, and Nathan Srebro.\n2015. In search of the real inductive bias: On the\nrole of implicit regularization in deep learning. In\nAdvances in Neural Information Processing Systems\n(NeurIPS) .\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, and 1\n\n--- Page 11 ---\nothers. 2022. Training language models to follow in-\nstructions with human feedback. Advances in neural\ninformation processing systems , 35:27730\u201327744.\nJupinder Parmar, Sanjev Satheesh, Mostofa Patwary,\nMohammad Shoeybi, and Bryan Catanzaro. 2024.\nReuse, don\u2019t retrain: A recipe for continued pre-\ntraining of language models. arXiv preprint\narXiv:2407.07263 .\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. Preprint , arXiv:2304.03277.\nJonas Pfeiffer, Ana\u00efs Glaude, Suchin Gururangan,\nXi Victoria Lin, Aishwarya Kamath, Andreas R\u00fcckl\u00e9,\nIvan Vuli \u00b4c, Iryna Gurevych, and Kyunghyun Cho.\n2020. Adapterfusion: Non-destructive task compo-\nsition for transfer learning. In European Conference\non Artificial Intelligence (ECAI) .\nZhen Qi, Jiajing Chen, Shuo Wang, Bingying Liu,\nHongye Zheng, and Chihang Wang. 2024. Op-\ntimizing multi-task learning for enhanced perfor-\nmance in large language models. arXiv preprint\narXiv:2412.06249 .\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research ,\n21(140):1\u201367.\nShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu,\nDuyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio\nBlanco, and Shuai Ma. 2020. Codebleu: a method\nfor automatic evaluation of code synthesis. arXiv\npreprint arXiv:2009.10297 .\nDavid Rolnick, Aanuj Ahuja, Jonathan Schwarz, Timo-\nthy Lillicrap, and Gregory Wayne. 2019. Experience\nreplay for continual learning. In Advances in Neural\nInformation Processing Systems (NeurIPS) .\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nand 1 others. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207 .\nJesus Solano, Mardhiyah Sanni, Oana-Maria Camburu,\nand Pasquale Minervini. 2024. Sparsefit: Few-shot\nprompting with sparse fine-tuning for jointly gener-\nating predictions and natural language explanations.\nPreprint , arXiv:2305.13235.\nJian Song, Di Liang, Rumei Li, Yuntao Li, Sirui Wang,\nMinlong Peng, Wei Wu, and Yongxin Yu. 2022.\nImproving semantic matching through dependency-\nenhanced pre-trained model with adaptive fusion.\narXiv preprint arXiv:2210.08471 .Ashton Stickland and Iain Murray. 2020. Bert and pals:\nProjected attention layers for efficient adaptation in\nmulti-task learning. In International Conference on\nMachine Learning (ICML) .\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya\nPathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,\nTatiana Matejovicova, Alexandre Ram\u00e9, Morgane\nRivi\u00e8re, Louis Rouillard, Thomas Mesnard, Geoffrey\nCideron, Jean bastien Grill, Sabela Ramos, Edouard\nYvinec, Michelle Casbon, Etienne Pot, Ivo Penchev,\nand 106 others. 2025. Gemma 3 technical report.\nPreprint , arXiv:2503.19786.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, and 1 others. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971 .\nSirui Wang, Di Liang, Jian Song, Yuntao Li, and\nWei Wu. 2022. Dabert: Dual attention en-\nhanced bert for semantic matching. arXiv preprint\narXiv:2210.03454 .\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652 .\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022. Finetuned\nlanguage models are zero-shot learners. Preprint ,\narXiv:2109.01652.\nMuling Wu, Qi Qian, Wenhao Liu, Xiaohua Wang, Zisu\nHuang, Di Liang, LI Miao, Shihan Dou, Changze\nLv, Zhenghua Wang, and 1 others. 2025a. Progres-\nsive mastery: Customized curriculum learning with\nguided prompting for mathematical reasoning. arXiv\npreprint arXiv:2506.04065 .\nTongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan,\nThuy-Trang Vu, and Gholamreza Haffari. 2024. Con-\ntinual learning for large language models: A survey.\nPreprint , arXiv:2402.01364.\nXianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Ji-\naheng Liu, Xeron Du, Di Liang, Daixin Shu, Xi-\nanfu Cheng, Tianzhen Sun, and 1 others. 2025b.\nTablebench: A comprehensive and complex bench-\nmark for table question answering. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 39, pages 25497\u201325506.\nXianjie Wu, Jian Yang, Tongliang Li, Shiwei Zhang,\nYiyang Du, LinZheng Chai, Di Liang, and Zhou-\njun Li. 2025c. Unleashing potential of evidence in\n\n--- Page 12 ---\nknowledge-intensive dialogue generation. In ICASSP\n2025-2025 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , pages\n1\u20135. IEEE.\nBenfeng Xu, Licheng Zhang, Zhendong Mao, Quan\nWang, Hongtao Xie, and Yongdong Zhang. 2020.\nCurriculum learning for natural language understand-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics , pages\n6095\u20136104, Online. Association for Computational\nLinguistics.\nChao Xue, Di Liang, Pengfei Wang, and Jing Zhang.\n2024. Question calibration and multi-hop modeling\nfor temporal question answering. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 38, pages 19332\u201319340.\nChao Xue, Di Liang, Sirui Wang, Jing Zhang, and Wei\nWu. 2023. Dual path modeling for semantic match-\ning by perceiving subtle conflicts. In ICASSP 2023-\n2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages 1\u20135.\nIEEE.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, Hao-\nran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian\nYang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, and\n43 others. 2024. Qwen2 technical report. Preprint ,\narXiv:2407.10671.\nEnneng Yang, Junwei Pan, Ximei Wang, Haibin Yu,\nLi Shen, Xihua Chen, Lei Xiao, Jie Jiang, and Guib-\ning Guo. 2023. Adatask: A task-aware adaptive\nlearning rate approach to multi-task learning. Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence , 37(9):10745\u201310753.\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey\nLevine, Karol Hausman, and Chelsea Finn. 2020.\nGradient surgery for multi-task learning. Advances\nin neural information processing systems , 33:5824\u2013\n5836.\nAmir Zamir, Alexander Sax, William Shen, Leonidas\nGuibas, Jitendra Malik, and Silvio Savarese. 2018.\nTaskonomy: Disentangling task transfer learning.\nPreprint , arXiv:1804.08328.\nSheng Zhang, Mohammad Norouzi, and Alexander\nKolesnikov. 2021. Revisiting multi-task learn-\ning in the deep learning age. arXiv preprint\narXiv:2105.02178 .\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,\nXiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-\nwei Zhang, Fei Wu, and Guoyin Wang. 2024. In-\nstruction tuning for large language models: A survey.\nPreprint , arXiv:2308.10792.\nXiao Zhang, Kangsheng Wang, Tianyu Hu, and Huimin\nMa. 2025. Efficient knowledge transfer in multi-task\nlearning through task-adaptive low-rank representa-\ntion. Preprint , arXiv:2505.00009.Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Zi Lin, Zhuohan Li, Dacheng\nLi, Eric Xing, and 1 others. 2023. Judging llm-as-a-\njudge with mt-bench and chatbot arena. Advances in\nNeural Information Processing Systems .\nRui Zheng, Rong Bao, Yuhao Zhou, Di Liang, Sirui\nWang, Wei Wu, Tao Gui, Qi Zhang, and Xuanjing\nHuang. 2022. Robust lottery tickets for pre-trained\nlanguage models. arXiv preprint arXiv:2211.03013 .\nA Experiments Setup\nWe conducted extensive experiments to evaluate the\neffectiveness of the proposed Dynamic Parameter\nIsolation (DPI) framework. The primary goals of\nour evaluation are to determine whether DPI outper-\nforms standard supervised fine-tuning (SFT) base-\nlines, including multi-task and multi-stage meth-\nods, across diverse and conflicting tasks; to assess\nDPI\u2019s ability to mitigate the \"seesaw effect\" and\ncatastrophic forgetting; to examine the sensitivity\nof DPI to hyperparameters such as the core percent-\nage (p) and similarity threshold ( \u03c4); and to analyze\nthe impact of its dynamic freezing mechanism.\nDatasets: We evaluate DPI on a diverse suite of\npublicly available datasets that represent struc-\ntured reasoning, code generation, and open-ended\ninstruction-following tasks. For mathematical rea-\nsoning, we use GSM8K (Cobbe et al., 2021), which\nevaluates multi-step reasoning through accuracy.\nFor code generation, we use CodeAlpaca (Chaud-\nhary, 2023), where performance is measured using\nCodeBLEU (Ren et al., 2020). For logical reason-\ning, we use LogiQA (Liu et al., 2020), which as-\nsesses logical consistency through accuracy scores.\nFor general instruction-following and conversa-\ntional abilities, we use Alpaca (Taori et al., 2023)\nand UltraChat (Ding et al., 2023), both evalu-\nated using GPT-4-based scoring on a 1-to-10 scale\n(Zheng et al., 2023). These datasets include a\nmix of structured tasks (e.g., GSM8K, LogiQA,\nCodeAlpaca) and open-ended tasks (e.g., Alpaca,\nUltraChat) to introduce potential conflicts in pa-\nrameter specialization. Each task is evaluated us-\ning its standard metric: accuracy for GSM8K and\nLogiQA, CodeBLEU for CodeAlpaca, and GPT-4\nscoring for Alpaca and UltraChat. To provide a\nunified comparison across tasks, we also report a\nmacro-average score ( Avg. Norm. Score ) by nor-\nmalizing individual task scores to a common 0-10\nscale.\nBaselines: We compare DPI against three SFT base-\nlines. (1) Full Multi-task SFT, where the model is\n\n--- Page 13 ---\nfine-tuned on a uniform mixture of all datasets with-\nout task grouping or parameter isolation. (2) Multi-\nStage SFT (Random Grouping), where tasks are\nrandomly partitioned into K= 3stages and fine-\ntuned sequentially, updating all parameters across\neach stage. (3) Multi-Stage SFT (Heuristic Group-\ning), where tasks grouped based on perceived simi-\nlarity (e.g., reasoning tasks grouped together, open-\nended tasks grouped together) are fine-tuned se-\nquentially over two stages, with all parameters up-\ndated during each stage.\nImplement details: All experiments use the\nLLaMA-2-7B (Touvron et al., 2023), Mistral-8B\n(Jiang et al., 2024), Gemma-9B (Team et al., 2025),\nand Qwen2-7B (Yang et al., 2024) as the base lan-\nguage model. Fine-tuning is performed using the\nAdamW optimizer (Loshchilov and Hutter, 2017)\nwith a learning rate of 1\u00d710\u22125, batch size of\n64, and a cosine learning rate schedule with 3%\nwarm-up steps. The main SFT stages involve train-\ning for three epochs on the datasets for each stage.\nFor DPI, core parameter identification is conducted\nthrough probe fine-tuning runs lasting one epoch\nper task ( Eprobe= 1). DPI hyperparameters are set\nto a core percentage of p= 1% and a similarity\nthreshold \u03c4= 0.1. Task groups derived by DPI are\nrandomly ordered for multi-stage training. Masked\nfine-tuning is applied during each stage, leveraging\nthe dynamic freezing mechanism to preserve task-\nspecific core parameter regions. All experiments\nare performed on machines equipped with eight\nNVIDIA A100 GPUs (80GB).\nB Related Work\nB.1 Supervised Fine-Tuning and Instruction\nTuning\nSupervised Fine-Tuning (SFT) is a prevalent tech-\nnique for specializing pre-trained LLMs (Brown\net al., 2020; Devlin et al., 2019; Raffel et al., 2020)\nfor desired downstream behaviors. Instruction tun-\ning (Wei et al., 2021; Sanh et al., 2021; Chung et al.,\n2024; Zheng et al., 2022; Ma et al., 2022; Wu et al.,\n2025a; Ouyang et al., 2022; Zhang et al., 2024;\nAnthropic, 2024; Peng et al., 2023; Fei et al., 2022;\nLiu et al., 2023a,b; Xue et al., 2024) a specific form\nof SFT, leverages datasets formatted as instructions\nand responses to enhance model controllability and\ngeneralization to unseen tasks. Standard SFT often\ninvolves training on a mixture of data from various\ntasks (Longpre et al., 2023; Chung et al., 2022; Wei\net al., 2022; Lu et al., 2023; Wu et al., 2025b,c),typically applying updates across the entire param-\neter space. While effective for general adaptation,\nthis approach can struggle when task objectives\nwithin the SFT data mixture conflict, leading to the\nperformance trade-offs (\"seesaw effect\") discussed\nin Section 1. Our work diverges from this standard\npractice by proposing a method to selectively up-\ndate parameters based on their identified relevance\nto specific tasks within the SFT process, thereby\ndirectly addressing the negative consequences of\nindiscriminate parameter updates.\nB.2 Task Interference and Knowledge\nRetention\nTask interference has been a persistent challenge\nin multi-task and sequential learning paradigms.\nSpecifically, training sequentially on multiple tasks\noften leads to catastrophic forgetting (McCloskey\nand Cohen, 1989; Wu et al., 2024), where knowl-\nedge acquired in earlier stages is overwritten or\ndegraded by subsequent updates. This problem has\nbeen studied extensively in the context of smaller\nneural architectures and motivated approaches such\nas regularization (Kirkpatrick et al., 2017; Zhang\net al., 2025), replay-based methods (Rolnick et al.,\n2019; Liu and Wang, 2024), and episodic mem-\nory mechanisms (L\u00f3pez-Paz and Ranzato, 2017;\nGuo et al., 2018; Zamir et al., 2018). While effec-\ntive for small-scale models, extending these tech-\nniques to the massive parameter spaces of LLMs is\nnon-trivial. Gradient-based methods have gained\npopularity for balancing task objectives. Grad-\nNorm (Chen et al., 2018; Lopez-Paz and Ran-\nzato, 2022) adjusts task-specific gradient magni-\ntudes, while PCGrad (Yu et al., 2020; Yang et al.,\n2023) selectively projects conflicting gradients to\nmitigate interference during multi-task learning.\nThese approaches focus primarily on optimizing\ntask gradients without addressing the root cause\nof parameter-level contention. Modular solutions,\nsuch as adapter fusion (Pfeiffer et al., 2020; Fedus\net al., 2022; Xu et al., 2020; Li and Hoiem, 2017;\nLiang et al., 2019b; Wang et al., 2022; Liang et al.,\n2019a), assign independent modules to tasks, allow-\ning architectural separation. Despite their advan-\ntages, such methods introduce added complexity\nand may not scale gracefully to hundreds of tasks.\nOur work departs from these paradigms by taking a\nparameter-centric approach. Rather than manipulat-\ning gradients or enforcing modularity, DPI directly\nquantifies and isolates the \u201ccore parameters\u201d for\neach task based on update magnitudes.\n\n--- Page 14 ---\nFigure 2: Ablation study on the similarity threshold \u03c4\nacross different base models (with p= 5% ). Scores are\nAvg. Norm. Score.\nB.3 Multi-Stage Fine-Tuning and Dynamic\nScheduling\nMulti-stage fine-tuning frameworks have been\nwidely adopted to tackle task heterogeneity in su-\npervised tuning scenarios. These approaches of-\nten heuristically group tasks into stages based on\nshared characteristics, such as similarity or diffi-\nculty, and train sequentially across multiple phases\n(Ouyang et al., 2022; Wei et al., 2021). While multi-\nstage frameworks can mitigate direct gradient con-\nflict by separating tasks temporally, they do not\naccount for overlaps in shared parameter usage. As\na result, tasks in later stages can destructively over-\nwrite knowledge embedded in parameters critical to\nearlier tasks, exacerbating catastrophic forgetting.\nMulti-task fine-tuning strategies emphasize con-\ncurrent training on several tasks to enable shared\nrepresentations (Caruana, 1997). However, multi-\ntask learning encounters challenges in balancing\ncompeting task gradients due to loss imbalance\nor gradient directionality. Techniques combining\nshared and task-specific spaces, such as (Stickland\nand Murray, 2020) and (Zhang et al., 2021), at-\ntempt to allocate independent regions of the model\nto different tasks. These approaches maintain task\nseparation but often constrain model capacity and\nreduce the ability to leverage shared knowledge\nacross tasks effectively. Dynamic task schedul-\ning has emerged as a promising approach to im-\nprove multi-task and multi-stage fine-tuning. For\ninstance, (Chen et al., 2020) proposed task prior-\nitization based on difficulty, enabling the model\nto iteratively refine its understanding across task\nsequences. Similarly, (Aribandi et al., 2021) pre-\nsented heuristic strategies for task grouping and\nordering to reduce task conflict. Although these\nmethods improve task alignment through betterscheduling, they still treat task interactions primar-\nily at a coarse data level and do not address task-\nspecific parameter differentiation within the LLMs.\nOur approach, Dynamic Parameter Isolation (DPI),\nextends beyond these advancements by perform-\ning explicit parameter-level disentanglement. Un-\nlike static task scheduling, our method dynamically\nidentifies and freezes task-specific parameter re-\ngions during multi-stage fine-tuning. Tasks with\noverlapping parameter regions are grouped into\njoint training stages to maximize synergy, while\ndisjoint tasks are staged sequentially with frozen\ncore parameters from earlier stages. By aligning\ntask scheduling explicitly with parameter sensitiv-\nity, DPI substantially mitigates destructive interfer-\nence without the need for heuristic task grouping\nor modular constraints.\nB.4 Parameter Heterogeneity and Isolation\nThe notion of parameter heterogeneity, where\ndifferent parameters within a model contribute\ndisproportionately to learning specific tasks, has\nbeen explored in the contexts of orthogonal\nweights, sparse updates, and parameter sharing\n(Neyshabur et al., 2015; Frankle and Carbin, 2019;\nMu and Lin, 2025). Inspired by these find-\nings, parameter-efficient fine-tuning methods like\nadapters (Houlsby et al., 2019; Frantar and Alis-\ntarh, 2023) and LoRA (Hu et al., 2021; Solano\net al., 2024; Song et al., 2022; Liu et al., 2024; Xue\net al., 2023; Li et al., 2024b,a) leverage this hetero-\ngeneity by introducing task-dedicated parameter\nsubspaces. Such methods demonstrate that task-\nspecific isolation can effectively reduce interfer-\nence, but they generally require additional param-\neters, limiting scalability in resource-constrained\napplications. A related line of research investigates\nparameter reuse and specialization within transfer\nlearning and continual learning. (Parmar et al.,\n2024; Li and Liang, 2021) explored weight spe-\ncialization during pretraining and task adaptation\nbut did not explicitly quantify task-specific param-\neter regions. Conceptually closer to our work, (Qi\net al., 2024) introduced task-sensitive routing to\npartition parameter updates among tasks dynami-\ncally. While this approach focuses on modular task\nrouting, our framework operates directly on model\nparameter sensitivity and exploits organic updates\nof pre-trained LLMs. DPI introduces a principled,\ndata-driven approach to identifying and preserv-\ning task-specific core parameter regions within the\nsame model architecture.",
  "project_dir": "artifacts/projects/enhanced_cs.CL_2508.21741v1_Not_All_Parameters_Are_Created_Equal_Smart_Isolat",
  "communication_dir": "artifacts/projects/enhanced_cs.CL_2508.21741v1_Not_All_Parameters_Are_Created_Equal_Smart_Isolat/.agent_comm",
  "assigned_at": "2025-09-02T20:48:22.092867",
  "status": "assigned"
}